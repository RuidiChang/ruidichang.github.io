<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html>
<head>
	<div class="navbar">
		<a href="./index.html"><font size="+1">Home</font></a>
	</div>
    <title>Ruidi Chang</title>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
    <link rel="stylesheet" type="text/css" href="style.css"/>
</head>
<body style="color: rgba(0, 0, 0, 0.938);margin:0;padding:0">
<div id="wrapper">
  <div id="content-wrap">
    <div id="content">
      <div id="main">
        <tr>
        <p style="padding-right:30px;"> <img id="headshot" "float-right" align="right" alt="Ruidi Chang" src="ruidi2.jpg" width="180" height="185"/></p>

            <p><font size="+3"><strong>&nbsp;Ruidi Chang</strong></font></p>
            <p></p>
            <p style="line-height:90%"><font size="3"><a href="https://csweb.rice.edu/" style="color:#065F46;">&nbsp;&nbsp;Department of Computer Science</a></font></p>
            <p style="line-height:90%"><font size="3"><a href="https://www.rice.edu/" style="color:#065F46;">&nbsp;&nbsp;Rice University</a></font></p>
            <p style="line-height:90%"><font size="3"><strong>&nbsp;&nbsp;Address:</strong> Duncan Hall, 6100 Main St, Houston, TX 77005</font></p>
            <p style="line-height:90%"><font size="3"><strong>&nbsp;&nbsp;Email:</strong> rc151@rice.edu</font></p>
            <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
            &nbsp;&nbsp;&nbsp;&nbsp;<a href="https://scholar.google.com/citations?user=gquBcSkAAAAJ"><i class="ai ai-google-scholar ai-2x" style="font-size:26px; color:black">&nbsp;</i></a>
            <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
            <a href="https://github.com/RuidiChang"><i class="fa fa-github" style="font-size:26px ;color:black">&nbsp;</i></a>
            <a href="https://x.com/ChangRuidi53250"><i class="fa fa-twitter" style="font-size:26px ;color:black">&nbsp;</i></a>
	    <a href="https://www.linkedin.com/in/ruidi-chang/"><i class="fa fa-linkedin-square" style="font-size:26px ;color:black">&nbsp;</i></a>
        <!-- <p></p> -->

        </tr>
		<hr style="color:rgb(218, 218, 218);">

        <h1><a name="biography">About Me</a></h1>
        <p>
			I‚Äôm a second-year CS Ph.D. student at 
			<a href="https://csweb.rice.edu/" style="color:#065F46;">Rice University</a>, 
			advised by Prof. 
			<a href="https://hanjiechen.github.io/index.html" style="color:#065F46;">Hanjie Chen</a>. 
			I‚Äôm interested in interpretable machine learning‚Äîespecially understanding how language models behave and process information to make decisions. 
			Before Rice, I earned my master‚Äôs at 
			<a href="https://www.cmu.edu/" style="color:#065F46;">Carnegie Mellon University</a>.
		</p>
        <h1><a name="publication">Publications</a></h1>
			<ul>
			  <li>
			    <a href="https://arxiv.org/pdf/2507.05158" style="color:#065F46;">Steering Information Utility in Key-Value Memory for Language Model Post-Training</a><br>
			    <em>Chunyuan Deng, <b>Ruidi Chang</b>, Hanjie Chen</em><br>
			    <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2025.
			  </li>
			
			  <li>
			    <a href="https://arxiv.org/pdf/2506.06686" style="color:#065F46;">Learning Distribution-wise Control in Representation Space for Language Models</a><br>
			    <em>Chunyuan Deng, <b>Ruidi Chang</b>, Hanjie Chen</em><br>
			    <em>International Conference on Machine Learning (ICML)</em>, 2025.
			  </li>
			
			 <li>
				  <a href="https://arxiv.org/pdf/2501.16374" style="color:#065F46;">
				    SAFR: Neuron Redistribution for Interpretability
				  </a><br>
				  <em><b>Ruidi Chang</b>, Chunyuan Deng, Hanjie Chen</em><br>
				  <em>North American Chapter of the Association for Computational Linguistics (NAACL) Findings</em>, 2025.
				
				  <div style="
				    margin-top:12px;
				    padding:15px;
				    border:1px solid #065F46;
				    border-radius:10px;
				    background-color:#f9fafb;
				    box-shadow:0 2px 6px rgba(0,0,0,0.05);
				    display:flex;
				    align-items:flex-start;
				    gap:15px;
				  ">
				    <!-- Text left -->
				    <div style="flex:1;">
				      <p style="margin:0 0 6px 0; line-height:1.35;">
				        üß† Neurons often mix too many features (superposition) ‚Äî making models a black box.<br>
				        üéØ <strong>SAFR</strong> strategically redistributes neurons:
				      </p>
				      <ul style="margin:0; padding-left:20px; line-height:1.3;">
				        <li>üß© Monosemantic for important tokens</li>
				        <li>üîó Polysemantic where interactions matter</li>
				      </ul>
				    </div>
				
				    <!-- Figure right -->
				    <figure style="margin:0; flex:0 0 35%; text-align:center;">
				      <img src="SAFR_workflow.png" alt="Workflow of SAFR" style="max-width:100%; height:auto; border-radius:8px;">
				      <figcaption style="font-size:0.8em; color:#065F46; margin-top:4px;">
				        <em>SAFR improves interpretability.</em>
				      </figcaption>
				    </figure>
				  </div>
				</li>
				
				
				<li>
				  <a href="https://arxiv.org/pdf/2507.05387" style="color:#065F46;">
				    The Generalization Ridge: Information Flow in Natural Language Generation
				  </a><br>
				  <em><b>Ruidi Chang</b>, Chunyuan Deng, Hanjie Chen</em><br>
				  <em>arXiv preprint, 2025</em>
				
				  <div style="
				    margin-top:12px;
				    padding:15px;
				    border:1px solid #065F46;
				    border-radius:10px;
				    background-color:#f9fafb;
				    box-shadow:0 2px 6px rgba(0,0,0,0.05);
				  ">
				    <!-- Text first -->
				    <p style="margin:0 0 6px 0; line-height:1.35;">
				      üîç Models aren‚Äôt just monotonically better deeper, they show a <strong>ridge of generalization</strong> in intermidiate layers.<br>
				      ‚úÇÔ∏è <strong>InfoRidge</strong> is an information-theoretic lens to trace how predictive information flows across depth.
				    </p>
				    <ul style="margin:0 8px 8px 20px; line-height:1.3;">
				      <li> Predictive information peaks in upper-middle layers (the ‚Äúridge‚Äù) then drops in final layers.</li>
				      <li> Residual scaling probes show that under distribution shift, models downweight deep layers and rely more on ridge layers.</li>
				    </ul>
				
				    <!-- Figure bottom -->
				    <figure style="margin:10px 0 0 0; text-align:center;">
				      <img src="InfoRidge_workflow.png" alt="Generalization Ridge diagram" style="max-width:80%; height:auto; border-radius:8px;">
				      <figcaption style="font-size:0.8em; color:#065F46; margin-top:4px;">
				        <em>InfoRidge identifies the generalization ridge ‚Äî a intermidiate-layer peak in predictive information.</em>
				      </figcaption>
				    </figure>
				  </div>
				</li>
				
				
				<li>
				  <a href="#" style="color:#065F46;">
				    Hidden Markov Modeling of Reasoning Dynamics in Large Language Models
				  </a><br>
				  <em><b>Ruidi Chang</b>, Jiawei Zhou, Hanjie Chen</em><br>
				  <em>In submission (ICLR 2026).</em>
				
				  <div style="
				    margin-top:12px;
				    padding:15px;
				    border:1px solid #065F46;
				    border-radius:10px;
				    background-color:#f9fafb;
				    box-shadow:0 2px 6px rgba(0,0,0,0.05);
				  ">
				    <p style="margin:0; line-height:1.35;">
				      üîÑ Reasoning in LLMs unfolds across both <strong>explicit steps</strong> in text and <strong>implicit shifts</strong> in hidden states. <br>
					  üèõÔ∏è Our <strong>Hierarchical Hidden Markov Model (HHMM)</strong> links these two views, modeling reasoning as trajectories through semantic roles and structural depth regimes.
				    </p>
				  </div>
				</li>
			
			  <li>
			    <a href="https://arxiv.org/pdf/2402.01680" style="color:#065F46;">Large Language Model Based Multi-Agents: A Survey of Progress and Challenges</a><br>
			    <em>Taicheng Guo, Xiuying Chen, Yaqi Wang, <b>Ruidi Chang</b>, Shichao Pei, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang</em><br>
			    <em>International Joint Conference on Artificial Intelligence (IJCAI)</em>, 2024.
			  </li>
			
			  <li>
			    <a href="https://arxiv.org/pdf/2410.15580" style="color:#065F46;">Language Models are Symbolic Learners in Arithmetic</a><br>
			    <em>Chunyuan Deng, Zhiqi Li, Roy Xie, <b>Ruidi Chang</b>, Hanjie Chen</em><br>
			    <em>In submission (Transactions on Machine Learning Research, TMLR).</em>
			  </li>
			</ul>

	<h1><a name="services">Services</a></h1>
          <ul>
                <li> <b>Reviewer:</b> ACL ARR 2025 Feb, IEEE BigData 2025, EMNLP BlackboxNLP Workshop 2025/2024, COLM 2025 Workshop XLLM-Reason-Plan, COLING 2025
                <li> <b>Volunteer:</b> EMNLP BlackboxNLP Workshop 2024
				<li> <b>Mentoring:</b> Nursultan Asilbekov (SURF Program)
              </ul>
          </ul> 
	      
		<hr style="color:rgb(218, 218, 218);">
		<div style="text-align: center; font-size: 18px;"><small>Last update: 10/2025</small>
		</div>
		<br>
      </div>
    </div>
  </div>
</div>

</body>
</html>
